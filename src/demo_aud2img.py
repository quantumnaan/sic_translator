"""

ç›®çš„:
ãƒ•ã‚§ãƒ¼ã‚ºAã¨ãƒ•ã‚§ãƒ¼ã‚ºBã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€å…¥åŠ›ã•ã‚ŒãŸéŸ³å£°ã‚¯ãƒªãƒƒãƒ—ã®
æ„å‘³ã‚’ç†è§£ã—ã€ãã‚Œã«æœ€ã‚‚è¿‘ã„ç”»åƒã‚’ç”»åƒã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‹ã‚‰æ¤œç´¢ã—ã¦è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¢ã€‚

å®Ÿè¡Œä¾‹:
python src/demo_aud2img.py \
    --acoustic_model_path models/acoustic_unit_model/ \
    --model_dir models/semantic_core_model/ \
    --image_gallery_dir data/images/ \
    --query_audio_file data/audio/10815824_2997e03d76.wav \
    --top_k 5
"""

import argparse
import os
import glob
from tqdm import tqdm

import torch
import torchaudio
import joblib
from PIL import Image
import matplotlib.pyplot as plt
import torch.nn.functional as F
import json
from transformers import Wav2Vec2Model

# ä»¥å‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚„é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from model import MultimodalAcousticModel 
from dataloader import get_transforms

def get_args():
    parser = argparse.ArgumentParser(description="Demo for audio-to-image retrieval using Phase B model.")
    parser.add_argument("--acoustic_model_path", type=str, required=True, help="...")
    parser.add_argument("--model_dir", type=str, required=True, help="Path to the directory containing the semantic model and best_params.json.")
    parser.add_argument("--image_gallery_dir", type=str, required=True, help="...")
    parser.add_argument("--query_audio_file", type=str, required=True, help="...")
    parser.add_argument("--top_k", type=int, default=5, help="...")
    parser.add_argument("--sample_rate", type=int, default=16000, help="...")
    parser.add_argument("--vocab_size", type=int, default=512, help="...")
    return parser.parse_args()

@torch.no_grad()
def create_or_load_image_index(model, image_dir, index_path, device):
    """ç”»åƒã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®å…¨ç”»åƒã®æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä¿å­˜ãƒ»ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    if os.path.exists(index_path):
        print(f"Loading existing image index from {index_path}...")
        return torch.load(index_path)

    print(f"Creating new image index, saving to {index_path}...")
    model.to(device)
    model.eval()
    
    _, image_transform = get_transforms()
    image_paths = glob.glob(os.path.join(image_dir, "*.*"))
    image_embeddings = []
    valid_image_paths = []

    for img_path in tqdm(image_paths, desc="Indexing images"):
        try:
            image = Image.open(img_path).convert("RGB")
            image = image_transform(image).unsqueeze(0).to(device)
            
            # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
            img_emb = model.vision_encoder(image)
            image_embeddings.append(img_emb.cpu())
            valid_image_paths.append(img_path)
        except Exception as e:
            print(f"Warning: Could not process image {img_path}. Error: {e}")

    if not image_embeddings:
        raise ValueError("No valid images found to create an index.")
        
    index = {
        "paths": valid_image_paths,
        "embeddings": torch.cat(image_embeddings, dim=0)
    }
    torch.save(index, index_path)
    return index

@torch.no_grad()
def get_audio_embedding(audio_path, acoustic_base_model, kmeans_model, semantic_model, sample_rate, device):
    """å˜ä¸€ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€çµ‚çš„ãªæ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ã™ã‚‹"""
    acoustic_base_model.to(device)
    semantic_model.to(device)
    acoustic_base_model.eval()
    semantic_model.eval()

    # 1. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    waveform, sr = torchaudio.load(audio_path)
    if sr != sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)
        waveform = resampler(waveform)
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)
    waveform = waveform.to(device)

    # 2. éŸ³éŸ¿å˜ä½ç³»åˆ—ã«å¤‰æ› (ãƒ•ã‚§ãƒ¼ã‚ºAã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
    features = acoustic_base_model(waveform).last_hidden_state.squeeze(0)
    units = kmeans_model.predict(features.cpu().numpy())
    units_tensor = torch.LongTensor(units).unsqueeze(0).to(device)

    # 3. æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ› (ãƒ•ã‚§ãƒ¼ã‚ºBã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
    audio_embedding = semantic_model.audio_encoder(units_tensor)
    return audio_embedding

def display_results(query_path, result_paths, result_scores):
    """æ¤œç´¢çµæœã®ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹"""
    plt.figure(figsize=(20, 5))
    
    # æœ€åˆã®subplotã¯ã‚¯ã‚¨ãƒªéŸ³å£°ã‚’ç¤ºã™ï¼ˆä»Šå›ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ï¼‰
    plt.subplot(1, len(result_paths) + 1, 1)
    plt.text(0.5, 0.5, f"Query:\n{os.path.basename(query_path)}", ha='center', va='center', fontsize=12)
    plt.axis('off')

    for i, (path, score) in enumerate(zip(result_paths, result_scores)):
        img = Image.open(path).convert("RGB")
        plt.subplot(1, len(result_paths) + 1, i + 2)
        plt.imshow(img)
        plt.title(f"Score: {score:.4f}")
        plt.axis('off')
    
    plt.suptitle("Audio-to-Image Retrieval Results", fontsize=16)
    plt.tight_layout()
    plt.show()

def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # ğŸ’¥ã€ã“ã“ã‹ã‚‰ä¿®æ­£ã€‘JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    params_path = os.path.join(args.model_dir, "best_params.json")
    semantic_model_path = os.path.join(args.model_dir, "semantic_core_model.pth")
    
    try:
        with open(params_path, 'r') as f:
            best_params = json.load(f)
        print(f"Loaded best hyperparameters from {params_path}")
    except FileNotFoundError:
        print(f"Error: {params_path} not found. Please run train_multimodal.py first.")
        return

    # 1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ã™ã¹ã¦ãƒ­ãƒ¼ãƒ‰
    # ãƒ•ã‚§ãƒ¼ã‚ºAã®ãƒ¢ãƒ‡ãƒ«
    acoustic_base_model = Wav2Vec2Model.from_pretrained(args.acoustic_model_path)
    kmeans_path = os.path.join(args.acoustic_model_path, "kmeans_codebook.joblib")
    kmeans_model = joblib.load(kmeans_path)
    
    # ãƒ•ã‚§ãƒ¼ã‚ºBã®ãƒ¢ãƒ‡ãƒ«

    # ãƒ•ã‚§ãƒ¼ã‚ºBã®ãƒ¢ãƒ‡ãƒ«ã‚’ã€èª­ã¿è¾¼ã‚“ã ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆæœŸåŒ–
    semantic_model = MultimodalAcousticModel(
        vocab_size=args.vocab_size, 
        embedding_dim=best_params['embedding_dim'], 
        num_layers=best_params['audio_encoder_layers']
    )
    semantic_model.load_state_dict(torch.load(semantic_model_path, map_location=device))

    # 2. ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰
    index_path = os.path.join(os.path.dirname(args.model_dir), "image_index.pt")
    image_index = create_or_load_image_index(semantic_model, args.image_gallery_dir, index_path, device)
    
    # 3. ã‚¯ã‚¨ãƒªéŸ³å£°ã®æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
    print(f"Processing query audio: {args.query_audio_file}...")
    query_audio_emb = get_audio_embedding(
        args.query_audio_file, 
        acoustic_base_model, 
        kmeans_model, 
        semantic_model, 
        args.sample_rate, 
        device
    )

    # 4. æ¤œç´¢ã®å®Ÿè¡Œï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
    print("Searching for similar images...")
    image_embeddings = image_index["embeddings"].to(device)
    similarities = F.cosine_similarity(query_audio_emb, image_embeddings)
    
    # é¡ä¼¼åº¦ãŒé«˜ã„é †ã«ãƒˆãƒƒãƒ—Kã‚’å–å¾—
    top_k_scores, top_k_indices = torch.topk(similarities, args.top_k)

    result_paths = [image_index["paths"][i] for i in top_k_indices.cpu().numpy()]
    result_scores = top_k_scores.cpu().numpy()

    # 5. çµæœã®è¡¨ç¤º
    print("\n--- Top K Results ---")
    for i, (path, score) in enumerate(zip(result_paths, result_scores)):
        print(f"{i+1}: {os.path.basename(path)} (Similarity: {score:.4f})")
    
    display_results(args.query_audio_file, result_paths, result_scores)

if __name__ == "__main__":
    main()