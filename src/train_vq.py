# train_vq.py

"""
train_vq.py

ç›®çš„:
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®Wav2Vec2ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€
K-Meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’ç”¨ã„ã¦Vector Quantization (VQ) ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®
ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’å­¦ç¿’ã™ã‚‹ã€‚æœ€çµ‚çš„ã«ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨VQãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’
çµ„ã¿åˆã‚ã›ãŸã€ŒéŸ³éŸ¿å˜ä½ç™ºè¦‹ãƒ¢ãƒ‡ãƒ«ã€ã‚’ä¿å­˜ã™ã‚‹ã€‚

å®Ÿè¡Œä¾‹:
python train_vq.py \
  --base_model_path "models/aud_base_model/" \
  --data_dir "data/target_lang_unlabeled/" \
  --output_dir "models/acoustic_unit_model/" \
  --num_clusters 512 \
  --sample_rate 16000 \
  --max_files_for_kmeans 1000
"""

import argparse
import os
import glob
from tqdm import tqdm

import torch
import torchaudio
import numpy as np
from sklearn.cluster import MiniBatchKMeans
import joblib
from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor # ğŸ’¥ FeatureExtractorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ


def get_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã™ã‚‹"""
    parser = argparse.ArgumentParser(description="Train VQ layer using K-Means on extracted features.")
    parser.add_argument("--base_model_path", type=str, required=True, help="Path to the fine-tuned base model directory.")
    parser.add_argument("--data_dir", type=str, required=True, help="Directory containing unlabeled audio files (.wav).")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save the final acoustic unit model.")
    parser.add_argument("--num_clusters", type=int, default=512, help="Number of clusters for K-Means (codebook size).")
    parser.add_argument("--sample_rate", type=int, default=16000, help="Target sample rate for audio.")
    parser.add_argument("--max_files_for_kmeans", type=int, default=1000, help="Maximum number of audio files to use for K-Means training to save memory/time.")
    
    return parser.parse_args()

def load_audio(file_path, target_sample_rate):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹"""
    waveform, sample_rate = torchaudio.load(file_path)
    if sample_rate != target_sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)
        waveform = resampler(waveform)
    return waveform.squeeze(0)

@torch.no_grad()
def extract_features(model, data_dir, max_files, sample_rate, device):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€K-Meanså­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã€‚
    ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
    """
    model.to(device)
    model.eval()

    filepaths = glob.glob(os.path.join(data_dir, "*.wav"))
    if len(filepaths) > max_files:
        filepaths = np.random.choice(filepaths, max_files, replace=False)

    all_features = []
    print(f"Extracting features from {len(filepaths)} audio files...")
    for path in tqdm(filepaths):
        try:
            waveform = load_audio(path, sample_rate)
            # é•·ã™ãã‚‹éŸ³å£°ã¯ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆã“ã“ã§ã¯å˜ç´”åŒ–ã®ãŸã‚ã€å…ˆé ­éƒ¨åˆ†ã®ã¿ä½¿ç”¨ï¼‰
            max_len = sample_rate * 30 # 30ç§’
            if len(waveform) > max_len:
                waveform = waveform[:max_len]
            
            waveform = waveform.to(device)
            
            # ç‰¹å¾´é‡ã‚’æŠ½å‡º
            features = model(waveform.unsqueeze(0)).last_hidden_state.squeeze(0) # (time, hidden_dim)
            
            # å…¨ã¦ã®ç‰¹å¾´é‡ã‚’ä½¿ã†ã¨å·¨å¤§ã«ãªã‚‹ãŸã‚ã€å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€å¤§200å€‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if features.shape[0] > 200:
                indices = np.random.choice(features.shape[0], 200, replace=False)
                features = features[indices]

            all_features.append(features.cpu().numpy())

        except Exception as e:
            print(f"Warning: Skipping file {path} due to error: {e}")

    return np.concatenate(all_features, axis=0)

def main():
    args = get_args()
    os.makedirs(args.output_dir, exist_ok=True)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"Loading base model and feature extractor from {args.base_model_path}...")
    base_model = Wav2Vec2Model.from_pretrained(args.base_model_path)
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(args.base_model_path) # ğŸ’¥ã€è¿½åŠ ã€‘
    
    # 2. ç‰¹å¾´é‡ã‚’æŠ½å‡º
    feature_vectors = extract_features(
        model=base_model,
        data_dir=args.data_dir,
        max_files=args.max_files_for_kmeans,
        sample_rate=args.sample_rate,
        device=device
    )
    print(f"Extracted a total of {feature_vectors.shape[0]} feature vectors for clustering.")

    # 3. K-Meansã§ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’å­¦ç¿’
    print(f"Training K-Means with {args.num_clusters} clusters...")
    kmeans = MiniBatchKMeans(
        n_clusters=args.num_clusters,
        random_state=0,
        batch_size=256,
        verbose=1,
        max_iter=100,
    )
    kmeans.fit(feature_vectors)
    
    # K-Meansãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯æƒ…å ±ã‚’å«ã‚€ï¼‰ã‚’ä¿å­˜
    kmeans_path = os.path.join(args.output_dir, "kmeans_codebook.joblib")
    joblib.dump(kmeans, kmeans_path)
    print(f"K-Means model saved to {kmeans_path}")

    # 4. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’çµ±åˆã—ã¦æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    # transformersã¯VQãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ç›´æ¥ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ã“ã“ã§ã¯
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ï¼ˆK-Meansã®ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒï¼‰ã‚’åˆ¥ã€…ã«ä¿å­˜ã™ã‚‹ã€‚
    # æ¨è«–æ™‚ã«ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã™ã‚‹ã€‚
    base_model.save_pretrained(args.output_dir)
    feature_extractor.save_pretrained(args.output_dir) # ğŸ’¥ã€è¿½åŠ ã€‘
    print(f"Final base model saved to {args.output_dir}")
    print("\n--- Training VQ Layer Complete ---")
    print(f"The final acoustic unit model consists of two parts:")
    print(f"1. The base feature extractor: saved in '{args.output_dir}'")
    print(f"2. The codebook (from K-Means): saved as '{kmeans_path}'")
    print("In the next phase (Phase B), load both to convert audio into acoustic unit sequences.")

if __name__ == "__main__":
    main()