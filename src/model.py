# model.py
import torch
import torch.nn as nn
from transformers import Wav2Vec2Model, ViTModel
import torchaudio
from torchvision import models

class MultimodalModel(nn.Module):
    def __init__(self, audio_model_name, vision_model_name, embedding_dim):
        super(MultimodalModel, self).__init__()
        # CPUã§ã®å®Ÿè¡Œã‚’è€ƒæ…®ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«æ³¨æ„
        #self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_name)
        #self.vision_encoder = ViTModel.from_pretrained(vision_model_name)
        
        # CPUå‘ã‘ç°¡æ˜“ç‰ˆ: MFCC + LSTM
        self.audio_encoder = AudioLSTMEncoder(input_size=40, hidden_size=embedding_dim)
        # CPUå‘ã‘ç°¡æ˜“ç‰ˆ: äº‹å‰å­¦ç¿’æ¸ˆã¿ResNet
        self.vision_encoder = VisionResNetEncoder(embedding_dim=embedding_dim)

        # æœ€çµ‚çš„ãªç‰¹å¾´é‡ã‚’å…±é€šã®æ¬¡å…ƒã«å°„å½±ã™ã‚‹å±¤
        # audio_output_dim = self.audio_encoder.config.hidden_size # Wav2Vec2
        # vision_output_dim = self.vision_encoder.config.hidden_size # ViT
        # self.audio_projection = nn.Linear(audio_output_dim, embedding_dim)
        # self.vision_projection = nn.Linear(vision_output_dim, embedding_dim)

    def forward(self, audio_inputs=None, image_inputs=None):
        audio_features = None
        if audio_inputs is not None:
            audio_features = self.audio_encoder(audio_inputs)

        image_features = None
        if image_inputs is not None:
            image_features = self.vision_encoder(image_inputs)
            
        return audio_features, image_features

# CPUã§ç¾å®Ÿçš„ã«å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ç°¡æ˜“ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
class AudioLSTMEncoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.mfcc = torchaudio.transforms.MFCC(n_mfcc=input_size)
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
    
    def forward(self, waveform):
        # (batch, time) -> (batch, n_mfcc, time)
        mfccs = self.mfcc(waveform).transpose(1, 2)
        # (batch, time, n_mfcc)
        _, (hidden, _) = self.lstm(mfccs)
        return hidden.squeeze(0)

class VisionResNetEncoder(nn.Module):
    def __init__(self, embedding_dim):
        super().__init__()
        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        modules = list(resnet.children())[:-1] # FCå±¤ã‚’é™¤å»
        self.resnet = nn.Sequential(*modules)
        self.projection = nn.Linear(resnet.fc.in_features, embedding_dim)

    def forward(self, images):
        features = self.resnet(images).flatten(1)
        return self.projection(features)
    

# (VisionResNetEncoder ã‚„ MultimodalModel ã¯ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã®ã‚‚ã®ã‹ã‚‰æµç”¨ãƒ»ä¿®æ­£)

class AcousticUnitEncoder(nn.Module):
    """
    éŸ³éŸ¿å˜ä½ã®ç³»åˆ—ï¼ˆæ•´æ•°ã®ãƒªã‚¹ãƒˆï¼‰ã‚’å—ã‘å–ã‚Šã€æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.1):
        super().__init__()
        # Embeddingå±¤ï¼šæ•´æ•°ã®IDã‚’å¯†ãªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        # LSTMå±¤ï¼šãƒ™ã‚¯ãƒˆãƒ«ã®ç³»åˆ—ã‹ã‚‰æ–‡è„ˆã‚’å­¦ç¿’
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)

    def forward(self, unit_sequences):
        # unit_sequences: (batch_size, seq_len)
        embedded = self.embedding(unit_sequences)
        # embedded: (batch_size, seq_len, embedding_dim)
        
        # LSTMã¯æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ã‚’æ–‡å…¨ä½“ã®è¡¨ç¾ã¨ã—ã¦åˆ©ç”¨
        _, (hidden, _) = self.lstm(embedded)
        # hidden: (num_layers, batch_size, hidden_dim)
        
        # æœ€å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®éš ã‚ŒçŠ¶æ…‹ã‚’è¿”ã™
        return hidden[-1]

class MultimodalAcousticModel(nn.Module):
    """
    æ–°ã—ã„AcousticUnitEncoderã¨VisionEncoderã‚’çµ„ã¿åˆã‚ã›ãŸæœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«
    """
    def __init__(self, vocab_size, embedding_dim, num_layers):
        super().__init__()
        # ğŸ’¥ã€å¤‰æ›´ç‚¹ã€‘æ–°ã—ã„éŸ³å£°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨
        self.audio_encoder = AcousticUnitEncoder(
            vocab_size=vocab_size, 
            embedding_dim=embedding_dim,
            hidden_dim=embedding_dim, # LSTMã®éš ã‚Œå±¤ã‚µã‚¤ã‚ºã‚’å…±é€šã®æ¬¡å…ƒã«
            num_layers=num_layers
        )
        self.vision_encoder = VisionResNetEncoder(embedding_dim=embedding_dim)

    def forward(self, unit_sequences, images):
        audio_embedding = self.audio_encoder(unit_sequences)
        image_embedding = self.vision_encoder(images)
        
        return audio_embedding, image_embedding