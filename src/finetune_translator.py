# finetune_translator.py

"""
finetune_translator.py (ãƒ•ã‚§ãƒ¼ã‚ºC)

ç›®çš„:
ãƒ•ã‚§ãƒ¼ã‚ºA, Bã§æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ã—ã¦çµ±åˆã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®
è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ã¨ã—ã¦æ¥ç¶šã™ã‚‹ã€‚
(éŸ³å£°, è‹±èªãƒ†ã‚­ã‚¹ãƒˆ)ã®å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ã“ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®
ç¿»è¨³ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚
"""

import warnings
from transformers.utils.logging import set_verbosity_error

# 1. transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªè‡ªä½“ã®ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æŠ‘åˆ¶
# ã“ã‚Œã«ã‚ˆã‚Šã€å¤šãã®transformersé–¢é€£ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒéè¡¨ç¤ºã«ãªã‚Šã¾ã™
set_verbosity_error()

# 2. ãã®ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆtorchaudioãªã©ï¼‰ã®ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æŠ‘åˆ¶
# FutureWarningã¨UserWarningã‚’ç„¡è¦–ã™ã‚‹ã‚ˆã†ã«è¨­å®š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import argparse
import os
import glob
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Union

import torch
import torch.nn as nn
import torchaudio
from torch.utils.data import Dataset
import joblib

from transformers import (
    Wav2Vec2Model,
    AutoTokenizer,
    AutoModelForCausalLM,
    EncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    Wav2Vec2FeatureExtractor,
    PreTrainedModel,
    Wav2Vec2Config,
    DataCollatorWithPadding,
    AutoConfig,
)
from transformers.modeling_outputs import BaseModelOutput

from model import AcousticUnitEncoder, MultimodalAcousticModel  # ãƒ•ã‚§ãƒ¼ã‚ºBã®model.pyã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# finetune_translator.py (e.g., after the imports)

from dataclasses import dataclass
from typing import Any, Dict, List, Union


def get_args():
    parser = argparse.ArgumentParser(
        description="Fine-tune the full translation model.")
    parser.add_argument("--acoustic_model_path", type=str, required=True)
    parser.add_argument("--semantic_model_path", type=str, required=True)
    parser.add_argument("--params_path", type=str, required=True)
    parser.add_argument("--parallel_data_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--decoder_model_name", type=str,
                        default="facebook/bart-base")
    parser.add_argument("--num_train_epochs", type=int, default=10)
    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument(
        "--resume_from_checkpoint",
        action="store_true",
        help="Resume training from the latest checkpoint in output_dir."
    )
    return parser.parse_args()


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Data collator that dynamically pads the inputs received.
    Args:
        feature_extractor ([`Wav2Vec2FeatureExtractor`])
            The feature extractor used for proccessing the data.
        tokenizer ([`PreTrainedTokenizer`])
            The tokenizer used for encoding the labels.
    """
    feature_extractor: Any
    tokenizer: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # Split inputs and labels since they have to be of different lengths
        # and need different padding methods.
        input_features = [{"input_values": feature["input_values"]}
                          for feature in features]
        label_features = [{"input_ids": feature["labels"]}
                          for feature in features]

        # Pad the audio inputs
        batch = self.feature_extractor.pad(
            input_features,
            padding=True,
            return_tensors="pt",
        )

        # Pad the text labels
        labels_batch = self.tokenizer.pad(
            label_features,
            padding=True,
            return_tensors="pt",
        )

        # Replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch


# --- Encoder: éŸ³å£°ã‹ã‚‰æ„å‘³è¡¨ç¾ã¸ ---

class SpeechToMeaningEncoder(PreTrainedModel):
    # PreTrainedModelã¨ã®äº’æ›æ€§ã®ãŸã‚ã«ã€config_classã‚’æŒ‡å®š
    config_class = Wav2Vec2Config

    # TrainerãŒå…¥åŠ›ã‚­ãƒ¼ã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«è¨­å®š
    main_input_name = "input_values"

    def __init__(self, config, decoder_config, acoustic_model_path, semantic_model_path, params_path):
        super().__init__(config)

        # 1. ãƒ•ã‚§ãƒ¼ã‚ºAãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        self.acoustic_base_model = Wav2Vec2Model.from_pretrained(
            acoustic_model_path)
        kmeans_path = os.path.join(
            acoustic_model_path, "kmeans_codebook.joblib")
        self.kmeans_model = joblib.load(kmeans_path)

        # 2. ãƒ•ã‚§ãƒ¼ã‚ºBãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        with open(params_path, 'r') as f:
            best_params = json.load(f)


        # ğŸ’¥ã€ä¿®æ­£ç‚¹ã€‘MultimodalAcousticModelã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ã€audio_encoderéƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨
        # ã“ã®éƒ¨åˆ†ã¯ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã›ãšã€ä¸‹ã§é‡ã¿ã‚’èª­ã¿è¾¼ã‚€
        self.semantic_encoder = AcousticUnitEncoder(
            vocab_size=best_params.get('vocab_size', 512),
            embedding_dim=best_params['embedding_dim'],
            hidden_dim=best_params['embedding_dim'],
            num_layers=best_params['audio_encoder_layers']
        )
        # è¦ªãƒ¢ãƒ‡ãƒ«ã®state_dictã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã™ã‚‹éƒ¨åˆ†ã ã‘ã‚’æŠ½å‡ºã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
        full_state_dict = torch.load(semantic_model_path, map_location="cpu")
        audio_encoder_state_dict = {
            key.replace("audio_encoder.", ""): value
            for key, value in full_state_dict.items()
            if key.startswith("audio_encoder.")
        }
        self.semantic_encoder.load_state_dict(audio_encoder_state_dict)
        self.projection = nn.Linear(best_params['embedding_dim'], decoder_config.d_model)

        # 3. å­¦ç¿’ä¸­ã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®é‡ã¿ã‚’å›ºå®šï¼ˆå‡çµï¼‰
        # æ–°ã—ã„ææ¡ˆï¼ˆéŸ³éŸ¿ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’å‡çµã—ã€æ„å‘³ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯å­¦ç¿’å¯¾è±¡ã«ã™ã‚‹ï¼‰
        print("Freezing acoustic_base_model parameters...")
        for param in self.acoustic_base_model.parameters():
            param.requires_grad = False
        
        # semantic_encoder ã¨ projection ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ requires_grad=True ãªã®ã§å­¦ç¿’ã•ã‚Œã‚‹
        # æ˜ç¤ºçš„ã«æ›¸ããªã‚‰ä»¥ä¸‹ã®é€šã‚Š
        print("Setting semantic_encoder and projection parameters to be trainable...")
        for param in self.semantic_encoder.parameters():
            param.requires_grad = True
        for param in self.projection.parameters():
            param.requires_grad = True

    def forward(self, input_values=None, **kwargs):
        # ğŸ’¥ã€æœ€é‡è¦ä¿®æ­£ç‚¹ã€‘self.deviceã«é ¼ã‚‰ãšã€å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ã™ã‚‹
        device = input_values.device

        # å„ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        self.acoustic_base_model.to(device)
        self.semantic_encoder.to(device)

        with torch.no_grad(): # acoustic_base_modelã¯å‡çµã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã“ã®éƒ¨åˆ†ã¯å‹¾é…è¨ˆç®—ä¸è¦
            features = self.acoustic_base_model(input_values).last_hidden_state

        semantic_outputs = []
        for i in range(features.shape[0]):  # ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†
            feats_np = features[i].cpu().detach().numpy()

            # ç‰¹å¾´é‡ -> éŸ³éŸ¿å˜ä½ç³»åˆ—
            units = self.kmeans_model.predict(feats_np)
            units_tensor = torch.LongTensor(units).unsqueeze(0).to(device)

            # éŸ³éŸ¿å˜ä½ç³»åˆ— -> æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆLSTMã®å…¨ç³»åˆ—ã®éš ã‚ŒçŠ¶æ…‹ã‚’å–å¾—ï¼‰
            semantic_output = self.semantic_encoder.lstm(
                self.semantic_encoder.embedding(units_tensor))[0]
            semantic_outputs.append(semantic_output)

        # æ³¨æ„: ãƒãƒƒãƒå†…ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦
        # ã“ã“ã§ã¯transformersã®EncoderDecoderModelãŒæ‰±ã„ã‚„ã™ã„ã‚ˆã†ã«æœ€çµ‚çš„ãªéš ã‚ŒçŠ¶æ…‹ã®ã¿ã‚’è¿”ã™
        # ã‚ˆã‚Šé«˜åº¦ãªå®Ÿè£…ã§ã¯ã“ã“ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ã‚’è¡Œã†
        final_hidden_states = torch.cat(
            [s.mean(dim=1) for s in semantic_outputs]).unsqueeze(1)

        with torch.enable_grad(): # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼å±¤ã®å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹
            projected_states = self.projection(final_hidden_states)
        # EncoderDecoderModelãŒå¿…è¦ã¨ã™ã‚‹å½¢å¼ã§ã€'last_hidden_state'ã‚­ãƒ¼ã¨å…±ã«è¿”ã™
        return BaseModelOutput(last_hidden_state=projected_states)


# --- ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
class ParallelDataset(Dataset):
    def __init__(self, data_dir, feature_extractor, tokenizer, max_text_length=128):
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.max_text_length = max_text_length

        audio_dir = os.path.join(data_dir, "audio")
        text_dir = os.path.join(data_dir, "text")
        self.pairs = []
        for audio_path in glob.glob(os.path.join(audio_dir, "*.wav")):
            base_name = os.path.splitext(os.path.basename(audio_path))[0]
            text_path = os.path.join(text_dir, f"{base_name}.txt")
            if os.path.exists(text_path):
                self.pairs.append({"audio": audio_path, "text": text_path})

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        pair = self.pairs[idx]

        # 1. éŸ³å£°ã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
        waveform, sr = torchaudio.load(pair["audio"])
        # feature_extractorã¯ãƒªã‚¹ãƒˆã§ã¯ãªãå˜ä¸€ã®æ³¢å½¢ã‚’å—ã‘å–ã‚Œã‚‹
        processed_audio = self.feature_extractor(
            waveform.squeeze(0).numpy(),  # NumPyé…åˆ—ã¨ã—ã¦æ¸¡ã™
            sampling_rate=self.feature_extractor.sampling_rate,
        )

        # 2. ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        with open(pair["text"], 'r', encoding='utf-8') as f:
            text = f.read().strip()

        tokenized_text = self.tokenizer(
            text,
            padding="max_length",  # ã“ã“ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            max_length=self.max_text_length,
            truncation=True,
        )

        # TrainerãŒå¿…è¦ã¨ã™ã‚‹ã‚­ãƒ¼åã§è¾æ›¸ã‚’è¿”ã™
        return {
            "input_values": torch.tensor(processed_audio.input_values[0]),
            "labels": torch.tensor(tokenized_text.input_ids)
        }

class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    """
    äºˆæœŸã—ãªã„å¼•æ•°ãŒãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã‚‹ã®ã‚’é˜²ããŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ Trainer
    """
    # ğŸ’¥ã€ä¿®æ­£ç‚¹ã€‘Traineræœ¬ä½“ã‹ã‚‰ã®å‘¼ã³å‡ºã—ã«åˆã‚ã›ã€å¼•æ•°ã« num_items_in_batch ã‚’è¿½åŠ 
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ num_items_in_batch ã‚’å—ã‘å–ã‚Šã¾ã™ãŒã€
        # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã«ã¯æ¸¡ã•ãªã„ã“ã¨ã§ã€ã“ã“ã§å¼•æ•°ã‚’å®‰å…¨ã«ã€Œæ¡ã‚Šã¤ã¶ã—ã€ã¾ã™ã€‚
        # è¦ªã‚¯ãƒ©ã‚¹ã® compute_loss ã‚’å‘¼ã³å‡ºã™éš›ã¯ã€ä½™åˆ†ãªå¼•æ•°ã‚’æ¸¡ã•ãªã„
        return super().compute_loss(model, inputs, return_outputs)


def main(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
    encoder_config = Wav2Vec2Config.from_pretrained(args.acoustic_model_path)
    decoder_config = AutoConfig.from_pretrained(args.decoder_model_name)
    encoder = SpeechToMeaningEncoder(
        config=encoder_config,
        decoder_config=decoder_config,
        acoustic_model_path=args.acoustic_model_path,
        semantic_model_path=args.semantic_model_path,
        params_path=args.params_path,
    )
    tokenizer = AutoTokenizer.from_pretrained(args.decoder_model_name)
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
        args.acoustic_model_path)

    # 2. Encoder-Decoderãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = EncoderDecoderModel.from_encoder_decoder_pretrained(
        encoder_pretrained_model_name_or_path=None,  # ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨
        decoder_pretrained_model_name_or_path=args.decoder_model_name,
        encoder_model=encoder
    )
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    model.config.decoder_start_token_id = tokenizer.bos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    
    model.tie_weights()

    # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’æº–å‚™
    dataset = ParallelDataset(args.parallel_data_dir,
                              feature_extractor, tokenizer)
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        feature_extractor=feature_extractor,
        tokenizer=tokenizer
    )

    # 4. å­¦ç¿’å¼•æ•°ã¨Trainerã‚’è¨­å®š
    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        learning_rate=args.learning_rate,
        fp16=torch.cuda.is_available(),
        save_total_limit=2,
        save_strategy="epoch",
        logging_strategy="epoch",
        remove_unused_columns=False,
    )

    trainer = CustomSeq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # 5. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    print("\n--- Starting Final Fine-tuning (Phase C) ---")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    print("\n--- Fine-tuning Complete ---")
    

    # 6. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    trainer.save_model(args.output_dir)
    feature_extractor.save_pretrained(args.output_dir)
    print(f"Final translator model saved to '{args.output_dir}'")


if __name__ == "__main__":
    args = get_args()
    main(args)
